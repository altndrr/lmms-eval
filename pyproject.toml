[tool.black]
line-length = 240

[build-system]
requires = ["setuptools>=42", "wheel", "setuptools_scm[tomli]>=6.3"]
build-backend = "setuptools.build_meta"

[project]
name = "lmms_eval"
version = "0.3.0"
authors = [
    { name = "LMMMs-Lab Evaluation Team", email = "lmms-lab@outlook.com" },
]
description = "A framework for evaluating large multi-modality language models"
readme = "README.md"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
requires-python = ">=3.8"
license = { text = "MIT" }

dependencies = [
    "accelerate>=0.29.1",
    "datasets>=3,<4",
    "evaluate>=0,<1",
    "torch>=2.1,<3",
    "torchvision>=0,<1",
    "transformers>=4,<5",
    # ------ misc ------
    "einops>=0,<1",
    "loguru>=0,<1",
    "numpy>=1,<3",
    "openai>=1,<2",
    "peft>=0,<1",
    "pytablewriter>=1,<2",
    "sacrebleu>=2,<3",
    "sentence_transformers>=3,<4",
    "sentencepiece>=0,<1",
    "sqlitedict>=2,<3",
    "tenacity>=9,<10",
    "wandb>=0,<1",
    # ------ models ------
    "qwen_vl_utils",
    # ------ tasks ------
    "anls",
    "capture_metric",
    "Levenshtein",
    "nltk",
    "pycocoevalcap",
    "rouge",
    "scikit-learn",
    "spacy",
]

[dependency-groups]
coco_cap = ["pycocoevalcap"]
convbench = ["anls"]
detailcaps = ["capture_metric", "pycocoevalcap"]
flickr30k = ["pycocoevalcap"]
llava_interleaved_bench = ["rouge"]
mathvista = ["Levenshtein"]
multidocvqa = ["Levenshtein"]
nocaps = ["pycocoevalcap"]
qwen_vl = ["qwen_vl_utils"]
refcoco = ["pycocoevalcap"]
refcoco_plus = ["pycocoevalcap"]
refcoco_g = ["pycocoevalcap"]
screenspot = ["pycocoevalcap"]
synthdog = ["nltk"]
textcaps = ["pycocoevalcap"]
vcr_wiki = ["nltk", "spacy"]
wild_vision_bench = ["scikit-learn"]

[project.optional-dependencies]
dev = ["black", "isort", "pre-commit"]

[tool.setuptools.packages.find]
include = ["lmms_eval*"]
exclude = [
    "assets*",
    "benchmark*",
    "docs",
    "dist*",
    "playground*",
    "scripts*",
    "tests*",
    "checkpoints*",
    "project_checkpoints*",
    "debug_checkpoints*",
    "mlx_configs*",
    "wandb*",
    "notebooks*",
    "logs*",
]

[tool.wheel]
exclude = [
    "assets*",
    "benchmark*",
    "docs",
    "dist*",
    "playground*",
    "scripts*",
    "tests*",
    "checkpoints*",
    "project_checkpoints*",
    "debug_checkpoints*",
    "mlx_configs*",
    "wandb*",
    "notebooks*",
    "logs*",
]

[project.scripts]
lmms-eval = "lmms_eval.__main__:cli_evaluate"

[project.urls]
Homepage = "https://lmms-lab.github.io"
Repository = "https://github.com/EvolvingLMMs-Lab/lmms-eval"
